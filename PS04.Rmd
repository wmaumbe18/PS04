---
title: "STAT/MATH 495: Problem Set 04"
author: "Wayne Maumbe"
date: "2017-10-03"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE)
set.seed(76)
```

# Load packages, data, model formulas

```{r, warning=FALSE}
library(tidyverse)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  select(-X1) %>%
  mutate(ID = 1:n()) %>% 
  select(ID, Balance, Income, Limit, Rating, Age, Cards, Education)
```
## Model

The following are the models trained on `credit_train`.

```{r}
model1_formula <- as.formula("Balance ~ 1")
model2_formula <- as.formula("Balance ~ Income")
model3_formula <- as.formula("Balance ~ Income + Limit")
model4_formula <- as.formula("Balance ~ Income + Limit + Rating")
model5_formula <- as.formula("Balance ~ Income + Limit + Rating + Age")
model6_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards")
model7_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards + Education")
```

... where `credit_train` is defined below, along with `credit_test`.

```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(20)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


# RMSE vs number of coefficients

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7. For now, I've filled them with arbitrary
# values; you will fill these in 
RMSE_train <- c(0,0,0,0,0,0,0)
RMSE_test <- c(1,1,1,1,1,1,1)



# 1. Fit a model ----------------------------------------
#create function that allows calculation for rmse as the mean of the squared difference of the predicted balance from the balance.  
modelfitfxn<-function(m,tr,ts){
  model <- lm(m, data=tr)
  pred <- model%>%
    broom::augment(newdata=ts)
  pred <- pred %>%
    mutate(sqdif=((pred$Balance-pred$.fitted)^2))
  rsme<-sqrt(mean(pred$sqdif))
  return(rsme)
}

#collect each model's rsme for both fitting on the test data and on the training data
RMSE_test<-c(
  modelfitfxn(model1_formula,credit_train,credit_test),
  modelfitfxn(model2_formula,credit_train,credit_test),
  modelfitfxn(model3_formula,credit_train,credit_test),
  modelfitfxn(model4_formula,credit_train,credit_test),
  modelfitfxn(model5_formula,credit_train,credit_test),
  modelfitfxn(model6_formula,credit_train,credit_test),
  modelfitfxn(model7_formula,credit_train,credit_test)
  )

RMSE_train<-c(
  modelfitfxn(model1_formula,credit_train,credit_train),
  modelfitfxn(model2_formula,credit_train,credit_train),
  modelfitfxn(model3_formula,credit_train,credit_train),
  modelfitfxn(model4_formula,credit_train,credit_train),
  modelfitfxn(model5_formula,credit_train,credit_train),
  modelfitfxn(model6_formula,credit_train,credit_train),
  modelfitfxn(model7_formula,credit_train,credit_train))
# Save results in a data frame. Note this data frame is in wide format.
results <- data_frame(
 num_coefficients = 1:7,
 RMSE_train,
 RMSE_test
 )

# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")

```


# Interpreting the graph

A lower RMSE value for a model means the model is better at prediction.
The graph above shows that for all the models the prediction is worse on the test data than on the training data.

For the test data, increasing the number of coefficients reduces the RMSE until there are 3 coefficients whereby any more increase in the number of coefficients is coupled with an increase in the RMSE. This could mean that the model does a good job estimating the signal as you increase the number of coefficients but there is a turning point whereby any increase in the number of coefficients  results in over-fitting. This means that the models with 1 and 2 coefficients are have a high RMSE because they are under-fitting whereas the increasing RMSE trend after adding 4 coefficients suggest the possibility of over-fitting.

The anomaly to this trend is that the RMSE value for 6 coefficients is lower than that for 5. This anomaly can be explained by considering the order in which the predictors are added to the model. This anomaly could be suggesting that Age is a better predictor because it lowers the RMSE.  

For the training data, increasing the number of coefficients decreases the RMSE. A notable feature of this trend is the the sharp decrease between 2 and 3 coefficients and relativity gradual decrease after 3 coefficients.This shows that as you increase the number of coefficients the model does a good job at estimating the signal until after adding 3 coefficients any more barely change the prediction power of the model.Like in the case for fitting on the test data, there is under-fitting when there is 1 and 2 coefficients in the model.

Also, the training data has RMSE consistently lower than that for the test data because of the Texas sharpshooter fallacy. This is because the models were created and fitted on the same data hence it is more likely to fit better on the training. 

An interesting observation is the sharp descend as you add the credit limit.This suggests that the credit limit is the best predictor as upon its addition there is sharp decrease in RMSE.     



# Bonus

The whole process is repeated, but her we let `credit_train` be a random sample of size 380
from `credit` instead of 20. Now compare and contrast this graph with the
one above and hypothesize as to the root cause of any differences.
```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(380)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


# RMSE vs number of coefficients

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7. For now, I've filled them with arbitrary 
# values; you will fill these in
RMSE_train <- c(0,0,0,0,0,0,0)
RMSE_test <- c(1,1,1,1,1,1,1)

#collect each model's rsme for both fitting on the test data and on the training data
RMSE_test<-c(
  modelfitfxn(model1_formula,credit_train,credit_test),
  modelfitfxn(model2_formula,credit_train,credit_test),
  modelfitfxn(model3_formula,credit_train,credit_test),
  modelfitfxn(model4_formula,credit_train,credit_test),
  modelfitfxn(model5_formula,credit_train,credit_test),
  modelfitfxn(model6_formula,credit_train,credit_test),
  modelfitfxn(model7_formula,credit_train,credit_test))

RMSE_train<-c(
  modelfitfxn(model1_formula,credit_train,credit_train),
  modelfitfxn(model2_formula,credit_train,credit_train),
  modelfitfxn(model3_formula,credit_train,credit_train),
  modelfitfxn(model4_formula,credit_train,credit_train),
  modelfitfxn(model5_formula,credit_train,credit_train),
  modelfitfxn(model6_formula,credit_train,credit_train),
  modelfitfxn(model7_formula,credit_train,credit_train))
# Save results in a data frame. Note this data frame is in wide format.
results <- data_frame(
 num_coefficients = 1:7,
 RMSE_train,
 RMSE_test
 )

# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")

```
Unlike in the first graph, this graph shows the RMSE value for the test data  starts off higher, and then goes lower than that of the training data after increasing number of coefficients over 3.

Like in the first case above, for both the training data and test data there is a sharp decrease in RMSE upon the addition of credit limit to the model. The hypothesis that after 3 coefficients any further addition of coefficients doesn't add to the prediction power of the model still applies in this case.Also hypothesis that for both the training and test data, there is under-fitting for the models with 1 and 2 coefficients.   

As sample size increases standard error decreases. In the case where the training data is 20 vs in that where it is 380, the standard error is larger.This means that there is more noise in this case hence it is more likely to fit to noise in the case of n=20 vs n=380 for the training set. This means that the model fit to a training set with n=380 is more 'generalizable' and estimates the signal better. Hence the reason why the RMSE value for the test data goes lower than that of the training data may be because the fit on the test data is better on second graph than the first. This is because the when the training set is larger the model fit estimates the signal better.

